{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqP7n664SPjO3qn0AI2fLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Senethlakshan/-hacktoberfest-2022/blob/main/data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frist Quection - done\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "employee = pd.read_csv('/content/dataset/Employee.csv')\n",
        "fields = pd.read_csv('/content/dataset/Fields.csv')\n",
        "workcode = pd.read_csv('/content/dataset/WorkCode.csv')\n",
        "workdetails = pd.read_csv('/content/dataset/WorkDetails.csv', low_memory=False)\n",
        "\n",
        "# Convert 'DateofJoin' to datetime objects, coercing errors to NaT\n",
        "employee['DateofJoin'] = pd.to_datetime(employee['DateofJoin'], errors='coerce')\n",
        "\n",
        "# Function to print dataset properties\n",
        "def print_dataset_properties(df, name):\n",
        "    print(f\"\\n=== {name} Properties ===\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\nUnique Values per Column:\")\n",
        "    for col in df.columns:\n",
        "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
        "    print(\"\\nSample Statistics:\")\n",
        "    print(df.describe(include='all'))\n",
        "\n",
        "# Analyze properties for each dataset\n",
        "print_dataset_properties(employee, \"Employee.csv\")\n",
        "print_dataset_properties(fields, \"Fields.csv\")\n",
        "print_dataset_properties(workcode, \"WorkCode.csv\")\n",
        "print_dataset_properties(workdetails, \"WorkDetails.csv\")\n",
        "\n",
        "# Specific analyses for Employee.csv\n",
        "print(\"\\n=== Employee.csv Specifics ===\")\n",
        "print(f\"Gender Distribution: \\n{employee['Gender'].value_counts(normalize=True)}\")\n",
        "print(f\"PAMACT Distribution: \\n{employee['PAMACT'].value_counts(dropna=False, normalize=True)}\")\n",
        "print(f\"DateofJoin Range: {employee['DateofJoin'].min()} to {employee['DateofJoin'].max()}\")\n",
        "print(f\"EmpCode Uniqueness: {'Unique' if employee['EmpCode'].is_unique else 'Non-unique'}\")\n",
        "\n",
        "# Specific analyses for Fields.csv\n",
        "print(\"\\n=== Fields.csv Specifics ===\")\n",
        "print(f\"Division Distribution: \\n{fields['Division'].value_counts(normalize=True)}\")\n",
        "print(f\"CropType Distribution: \\n{fields['CropType'].value_counts(normalize=True)}\")\n",
        "print(f\"Area (Ha) Summary: \\n{fields['Area (Ha)'].describe()}\")\n",
        "print(f\"ID Uniqueness: {'Unique' if fields['ID'].is_unique else 'Non-unique'}\")\n",
        "\n",
        "# Specific analyses for WorkCode.csv\n",
        "print(\"\\n=== WorkCode.csv Specifics ===\")\n",
        "print(f\"WorCode Uniqueness: {'Unique' if workcode['WorCode'].is_unique else 'Non-unique'}\")\n",
        "print(f\"Crop Distribution: \\n{workcode['Crop'].value_counts(dropna=False, normalize=True)}\")\n",
        "print(f\"Type Distribution: \\n{workcode['Type'].value_counts(dropna=False, normalize=True)}\")\n",
        "\n",
        "# Specific analyses for WorkDetails.csv\n",
        "print(\"\\n=== WorkDetails.csv Specifics ===\")\n",
        "print(f\"Year Range: {workdetails['Year'].min()} to {workdetails['Year'].max()}\")\n",
        "print(f\"Work Distribution (Top 5): \\n{workdetails['Work'].value_counts(normalize=True).head()}\")\n",
        "print(f\"Workdayfraction Summary: \\n{workdetails['Workdayfraction'].describe()}\")\n",
        "print(f\"Qty Missing Percentage: {workdetails['Qty'].isnull().mean()*100:.2f}%\")\n",
        "\n",
        "# Demonstrate Relationships\n",
        "print(\"\\n=== Relationships Analysis ===\")\n",
        "\n",
        "# 1. Employee to Fields (1:1 via EmpCode=ID)\n",
        "emp_fields = pd.merge(employee[['EmpCode', 'Gender']], fields[['ID', 'Division', 'CropType']],\n",
        "                     left_on='EmpCode', right_on='ID', how='inner')\n",
        "print(f\"\\nEmployee-Fields Merge (1:1): {emp_fields.shape[0]} rows (matches Employee/Fields row count)\")\n",
        "print(f\"Sample Employee-Fields Merge:\\n{emp_fields.head()}\")\n",
        "\n",
        "# 2. Employee to WorkDetails (1:M via EmpCode)\n",
        "emp_workdetails = pd.merge(workdetails[['EmpCode', 'Year', 'Month', 'Work', 'Qty']],\n",
        "                         employee[['EmpCode', 'Gender']],\n",
        "                         on='EmpCode', how='inner')\n",
        "print(f\"\\nEmployee-WorkDetails Merge (1:M): {emp_workdetails.shape[0]} rows\")\n",
        "print(f\"Average records per EmpCode: {emp_workdetails.groupby('EmpCode').size().mean():.2f}\")\n",
        "print(f\"Sample Employee-WorkDetails Merge:\\n{emp_workdetails.head()}\")\n",
        "\n",
        "# 3. WorkDetails to WorkCode (M:1 via Work=WorCode)\n",
        "workdetails_workcode = pd.merge(workdetails[['EmpCode', 'Work', 'Qty']],\n",
        "                               workcode[['WorCode', 'Crop', 'Type']],\n",
        "                               left_on='Work', right_on='WorCode', how='inner')\n",
        "print(f\"\\nWorkDetails-WorkCode Merge (M:1): {workdetails_workcode.shape[0]} rows\")\n",
        "print(f\"Sample WorkDetails-WorkCode Merge:\\n{workdetails_workcode.head()}\")\n",
        "\n",
        "# 4. Full Merge (Employee, Fields, WorkDetails, WorkCode)\n",
        "full_merge = pd.merge(pd.merge(pd.merge(workdetails,\n",
        "                                      employee[['EmpCode', 'Gender', 'DateofJoin']],\n",
        "                                      on='EmpCode', how='inner'),\n",
        "                             fields[['ID', 'Division', 'CropType', 'Area (Ha)']],\n",
        "                             left_on='EmpCode', right_on='ID', how='inner'),\n",
        "                     workcode[['WorCode', 'Crop', 'Type']],\n",
        "                     left_on='Work', right_on='WorCode', how='inner')\n",
        "print(f\"\\nFull Merge Result: {full_merge.shape}\")\n",
        "print(f\"Sample Full Merge:\\n{full_merge.head()}\")\n",
        "\n",
        "# Verify Division from Fields.csv (authoritative)\n",
        "print(f\"\\nDivision Source Check (Fields.csv):\")\n",
        "print(f\"Unique Divisions in Fields: {fields['Division'].unique()}\")\n",
        "print(f\"Division Distribution in Full Merge: \\n{full_merge['Division'].value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kshogtl_j8F",
        "outputId": "1eda510b-0469-4489-a987-673846c35c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Employee.csv Properties ===\n",
            "Shape: (1932, 5)\n",
            "\n",
            "Data Types:\n",
            "Estate                object\n",
            "EmpCode                int64\n",
            "Gender                object\n",
            "DateofJoin    datetime64[ns]\n",
            "PAMACT                object\n",
            "dtype: object\n",
            "\n",
            "Missing Values:\n",
            "Estate          0\n",
            "EmpCode         0\n",
            "Gender          0\n",
            "DateofJoin    166\n",
            "PAMACT          6\n",
            "dtype: int64\n",
            "\n",
            "Unique Values per Column:\n",
            "Estate: 1 unique values\n",
            "EmpCode: 1932 unique values\n",
            "Gender: 2 unique values\n",
            "DateofJoin: 713 unique values\n",
            "PAMACT: 4 unique values\n",
            "\n",
            "Sample Statistics:\n",
            "       Estate       EmpCode Gender                     DateofJoin PAMACT\n",
            "count    1932  1.932000e+03   1932                           1766   1926\n",
            "unique      1           NaN      2                            NaN      4\n",
            "top       EST           NaN      F                            NaN      A\n",
            "freq     1932           NaN   1071                            NaN   1404\n",
            "mean      NaN  7.305781e+05    NaN  1991-11-01 12:01:37.848244608    NaN\n",
            "min       NaN  1.010000e+02    NaN            1957-02-19 00:00:00    NaN\n",
            "25%       NaN  3.789268e+05    NaN            1983-10-14 12:00:00    NaN\n",
            "50%       NaN  5.474705e+05    NaN            1990-12-14 00:00:00    NaN\n",
            "75%       NaN  1.047395e+06    NaN            2002-01-08 00:00:00    NaN\n",
            "max       NaN  6.065353e+06    NaN            2011-11-03 00:00:00    NaN\n",
            "std       NaN  9.010979e+05    NaN                            NaN    NaN\n",
            "\n",
            "=== Fields.csv Properties ===\n",
            "Shape: (1932, 8)\n",
            "\n",
            "Data Types:\n",
            "ID                int64\n",
            "EState           object\n",
            "Division         object\n",
            "CropType         object\n",
            "Field            object\n",
            "Area (Ha)       float64\n",
            "NumberOfTree      int64\n",
            "Type             object\n",
            "dtype: object\n",
            "\n",
            "Missing Values:\n",
            "ID              0\n",
            "EState          0\n",
            "Division        0\n",
            "CropType        0\n",
            "Field           0\n",
            "Area (Ha)       0\n",
            "NumberOfTree    0\n",
            "Type            0\n",
            "dtype: int64\n",
            "\n",
            "Unique Values per Column:\n",
            "ID: 1932 unique values\n",
            "EState: 1 unique values\n",
            "Division: 5 unique values\n",
            "CropType: 3 unique values\n",
            "Field: 63 unique values\n",
            "Area (Ha): 18 unique values\n",
            "NumberOfTree: 34 unique values\n",
            "Type: 3 unique values\n",
            "\n",
            "Sample Statistics:\n",
            "                  ID EState Division CropType Field    Area (Ha)  \\\n",
            "count   1.932000e+03   1932     1932     1932  1932  1932.000000   \n",
            "unique           NaN      1        5        3    63          NaN   \n",
            "top              NaN    EST       UD        T   GEN          NaN   \n",
            "freq             NaN   1932      584     1863    74          NaN   \n",
            "mean    7.305781e+05    NaN      NaN      NaN   NaN     2.666718   \n",
            "std     9.010979e+05    NaN      NaN      NaN   NaN     3.043737   \n",
            "min     1.010000e+02    NaN      NaN      NaN   NaN     0.300000   \n",
            "25%     3.789268e+05    NaN      NaN      NaN   NaN     0.800000   \n",
            "50%     5.474705e+05    NaN      NaN      NaN   NaN     1.800000   \n",
            "75%     1.047395e+06    NaN      NaN      NaN   NaN     3.000000   \n",
            "max     6.065353e+06    NaN      NaN      NaN   NaN    16.000000   \n",
            "\n",
            "         NumberOfTree  Type  \n",
            "count     1932.000000  1932  \n",
            "unique            NaN     3  \n",
            "top               NaN     R  \n",
            "freq              NaN  1131  \n",
            "mean     23148.054348   NaN  \n",
            "std      22457.605353   NaN  \n",
            "min          0.000000   NaN  \n",
            "25%       9257.000000   NaN  \n",
            "50%      17631.000000   NaN  \n",
            "75%      29000.000000   NaN  \n",
            "max     124000.000000   NaN  \n",
            "\n",
            "=== WorkCode.csv Properties ===\n",
            "Shape: (483, 3)\n",
            "\n",
            "Data Types:\n",
            "WorCode    object\n",
            "Crop       object\n",
            "Type       object\n",
            "dtype: object\n",
            "\n",
            "Missing Values:\n",
            "WorCode    0\n",
            "Crop       0\n",
            "Type       4\n",
            "dtype: int64\n",
            "\n",
            "Unique Values per Column:\n",
            "WorCode: 481 unique values\n",
            "Crop: 1 unique values\n",
            "Type: 3 unique values\n",
            "\n",
            "Sample Statistics:\n",
            "       WorCode Crop Type\n",
            "count      483  483  479\n",
            "unique     481    1    3\n",
            "top        CT3    T    R\n",
            "freq         2  483  336\n",
            "\n",
            "=== WorkDetails.csv Properties ===\n",
            "Shape: (1048575, 7)\n",
            "\n",
            "Data Types:\n",
            "Year                 int64\n",
            "Month                int64\n",
            "EmpCode              int64\n",
            "Work                object\n",
            "Workdayfraction    float64\n",
            "Qty                float64\n",
            "ExtraKilos         float64\n",
            "dtype: object\n",
            "\n",
            "Missing Values:\n",
            "Year                    0\n",
            "Month                   0\n",
            "EmpCode                 0\n",
            "Work                    0\n",
            "Workdayfraction         0\n",
            "Qty                141028\n",
            "ExtraKilos          98824\n",
            "dtype: int64\n",
            "\n",
            "Unique Values per Column:\n",
            "Year: 4 unique values\n",
            "Month: 12 unique values\n",
            "EmpCode: 1561 unique values\n",
            "Work: 137 unique values\n",
            "Workdayfraction: 4 unique values\n",
            "Qty: 2411 unique values\n",
            "ExtraKilos: 428 unique values\n",
            "\n",
            "Sample Statistics:\n",
            "                Year         Month       EmpCode     Work  Workdayfraction  \\\n",
            "count   1.048575e+06  1.048575e+06  1.048575e+06  1048575     1.048575e+06   \n",
            "unique           NaN           NaN           NaN      137              NaN   \n",
            "top              NaN           NaN           NaN      ABS              NaN   \n",
            "freq             NaN           NaN           NaN   364458              NaN   \n",
            "mean    2.013353e+03  6.363929e+00  4.980240e+05      NaN     1.004997e+00   \n",
            "std     1.096421e+00  3.369590e+00  4.951038e+05      NaN     1.122915e-01   \n",
            "min     2.012000e+03  1.000000e+00  1.414000e+03      NaN     5.000000e-01   \n",
            "25%     2.012000e+03  3.000000e+00  1.229170e+05      NaN     1.000000e+00   \n",
            "50%     2.013000e+03  6.000000e+00  4.884360e+05      NaN     1.000000e+00   \n",
            "75%     2.014000e+03  9.000000e+00  5.803460e+05      NaN     1.000000e+00   \n",
            "max     2.015000e+03  1.200000e+01  6.065353e+06      NaN     1.500000e+00   \n",
            "\n",
            "                  Qty     ExtraKilos  \n",
            "count   907547.000000  949751.000000  \n",
            "unique            NaN            NaN  \n",
            "top               NaN            NaN  \n",
            "freq              NaN            NaN  \n",
            "mean        66.785474       8.818860  \n",
            "std        213.391730     168.633171  \n",
            "min         22.000000       0.000000  \n",
            "25%         34.000000       0.000000  \n",
            "50%         52.000000       0.000000  \n",
            "75%         71.000000       9.000000  \n",
            "max       4598.000000    8736.000000  \n",
            "\n",
            "=== Employee.csv Specifics ===\n",
            "Gender Distribution: \n",
            "Gender\n",
            "F    0.554348\n",
            "M    0.445652\n",
            "Name: proportion, dtype: float64\n",
            "PAMACT Distribution: \n",
            "PAMACT\n",
            "A      0.726708\n",
            "T      0.258282\n",
            "I      0.006211\n",
            "P      0.005694\n",
            "NaN    0.003106\n",
            "Name: proportion, dtype: float64\n",
            "DateofJoin Range: 1957-02-19 00:00:00 to 2011-11-03 00:00:00\n",
            "EmpCode Uniqueness: Unique\n",
            "\n",
            "=== Fields.csv Specifics ===\n",
            "Division Distribution: \n",
            "Division\n",
            "UD    0.302277\n",
            "HO    0.252588\n",
            "MD    0.193582\n",
            "ST    0.128882\n",
            "LD    0.122671\n",
            "Name: proportion, dtype: float64\n",
            "CropType Distribution: \n",
            "CropType\n",
            "T    0.964286\n",
            "V    0.018116\n",
            "B    0.017598\n",
            "Name: proportion, dtype: float64\n",
            "Area (Ha) Summary: \n",
            "count    1932.000000\n",
            "mean        2.666718\n",
            "std         3.043737\n",
            "min         0.300000\n",
            "25%         0.800000\n",
            "50%         1.800000\n",
            "75%         3.000000\n",
            "max        16.000000\n",
            "Name: Area (Ha), dtype: float64\n",
            "ID Uniqueness: Unique\n",
            "\n",
            "=== WorkCode.csv Specifics ===\n",
            "WorCode Uniqueness: Non-unique\n",
            "Crop Distribution: \n",
            "Crop\n",
            "T    1.0\n",
            "Name: proportion, dtype: float64\n",
            "Type Distribution: \n",
            "Type\n",
            "R      0.695652\n",
            "O      0.161491\n",
            "C      0.134576\n",
            "NaN    0.008282\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "=== WorkDetails.csv Specifics ===\n",
            "Year Range: 2012 to 2015\n",
            "Work Distribution (Top 5): \n",
            "Work\n",
            "ABS    0.347575\n",
            "PLK    0.322094\n",
            "LLO    0.080254\n",
            "XXX    0.056221\n",
            "XHP    0.029965\n",
            "Name: proportion, dtype: float64\n",
            "Workdayfraction Summary: \n",
            "count    1.048575e+06\n",
            "mean     1.004997e+00\n",
            "std      1.122915e-01\n",
            "min      5.000000e-01\n",
            "25%      1.000000e+00\n",
            "50%      1.000000e+00\n",
            "75%      1.000000e+00\n",
            "max      1.500000e+00\n",
            "Name: Workdayfraction, dtype: float64\n",
            "Qty Missing Percentage: 13.45%\n",
            "\n",
            "=== Relationships Analysis ===\n",
            "\n",
            "Employee-Fields Merge (1:1): 1932 rows (matches Employee/Fields row count)\n",
            "Sample Employee-Fields Merge:\n",
            "   EmpCode Gender     ID Division CropType\n",
            "0     2727      F   2727       HO        T\n",
            "1    11211      M  11211       HO        T\n",
            "2    29593      F  29593       HO        T\n",
            "3    37774      F  37774       HO        T\n",
            "4    37875      F  37875       HO        T\n",
            "\n",
            "Employee-WorkDetails Merge (1:M): 1048575 rows\n",
            "Average records per EmpCode: 671.73\n",
            "Sample Employee-WorkDetails Merge:\n",
            "   EmpCode  Year  Month Work   Qty Gender\n",
            "0    41107  2012      6  ABS  22.0      F\n",
            "1    41309  2012      7  PLK  22.0      F\n",
            "2    41309  2012      9  PLK  22.0      F\n",
            "3    41309  2014      1  PLK  22.0      F\n",
            "4    41309  2013      2  PLK  22.0      F\n",
            "\n",
            "WorkDetails-WorkCode Merge (M:1): 1048733 rows\n",
            "Sample WorkDetails-WorkCode Merge:\n",
            "   EmpCode Work   Qty WorCode Crop Type\n",
            "0    41107  ABS  22.0     ABS    T  NaN\n",
            "1    41309  PLK  22.0     PLK    T    R\n",
            "2    41309  PLK  22.0     PLK    T    R\n",
            "3    41309  PLK  22.0     PLK    T    R\n",
            "4    41309  PLK  22.0     PLK    T    R\n",
            "\n",
            "Full Merge Result: (1048733, 16)\n",
            "Sample Full Merge:\n",
            "   Year  Month  EmpCode Work  Workdayfraction   Qty  ExtraKilos Gender  \\\n",
            "0  2012      6    41107  ABS              1.0  22.0         0.0      F   \n",
            "1  2012      7    41309  PLK              1.0  22.0         0.0      F   \n",
            "2  2012      9    41309  PLK              1.0  22.0         0.0      F   \n",
            "3  2014      1    41309  PLK              1.0  22.0         0.0      F   \n",
            "4  2013      2    41309  PLK              1.5  22.0         0.0      F   \n",
            "\n",
            "  DateofJoin     ID Division CropType  Area (Ha) WorCode Crop Type  \n",
            "0 1973-05-01  41107       HO        T        2.0     ABS    T  NaN  \n",
            "1 1979-12-31  41309       HO        T        1.0     PLK    T    R  \n",
            "2 1979-12-31  41309       HO        T        1.0     PLK    T    R  \n",
            "3 1979-12-31  41309       HO        T        1.0     PLK    T    R  \n",
            "4 1979-12-31  41309       HO        T        1.0     PLK    T    R  \n",
            "\n",
            "Division Source Check (Fields.csv):\n",
            "Unique Divisions in Fields: ['HO' 'LD' 'MD' 'ST' 'UD']\n",
            "Division Distribution in Full Merge: \n",
            "Division\n",
            "HO    0.362028\n",
            "UD    0.203541\n",
            "MD    0.198702\n",
            "LD    0.158300\n",
            "ST    0.077430\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets from CSV files\n",
        "employee = pd.read_csv('/content/dataset/Employee.csv')\n",
        "fields = pd.read_csv('/content/dataset/Fields.csv')\n",
        "workcode = pd.read_csv('/content/dataset/WorkCode.csv')\n",
        "workdetails = pd.read_csv('/content/dataset/WorkDetails.csv', low_memory=False)  # Handle large file\n",
        "\n",
        "# Verify loading by printing shapes\n",
        "print(\"Dataset Shapes after Loading:\")\n",
        "print(f\"Employee.csv: {employee.shape}\")\n",
        "print(f\"Fields.csv: {fields.shape}\")\n",
        "print(f\"WorkCode.csv: {workcode.shape}\")\n",
        "print(f\"WorkDetails.csv: {workdetails.shape}\")\n",
        "\n",
        "# Optional: Display basic info for confirmation\n",
        "print(\"\\nBasic Info for Employee.csv:\")\n",
        "print(employee.info())\n",
        "print(\"\\nFirst few rows of Employee.csv:\")\n",
        "print(employee.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "yrEoJ_AJfF8P",
        "outputId": "aafdc3d5-0c01-4700-8b44-9cf0827a682e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 7 fields in line 78912, saw 8\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-916196758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset/Fields.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mworkcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset/WorkCode.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mworkdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset/WorkDetails.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle large file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Verify loading by printing shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 78912, saw 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Try reading up to the problematic line (add a small buffer for context)\n",
        "try:\n",
        "    problematic_data = pd.read_csv('/content/dataset/WorkDetails.csv', nrows=78912 + 10, low_memory=False)\n",
        "    print(\"Successfully read up to line 78922\")\n",
        "    print(problematic_data.tail())  # Show last few rows\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error at: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sowcUTHbhcr-",
        "outputId": "d2c011e3-6152-4047-9a2d-a06133b2588d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error at: Error tokenizing data. C error: Expected 7 fields in line 78912, saw 8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file as text and check line 78912 (lines are 1-indexed, so index 78911)\n",
        "with open('/content/dataset/WorkDetails.csv', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    print(\"Line 78911 (0-indexed):\", lines[78911].strip())  # The problematic line\n",
        "    print(\"Line 78910 (previous):\", lines[78910].strip() if len(lines) > 78910 else \"N/A\")\n",
        "    print(\"Line 78912 (next):\", lines[78912].strip() if len(lines) > 78912 else \"N/A\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLxnJdh7hi8I",
        "outputId": "2966d9f3-4f58-44cd-ff08-b271a74cc6e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line 78911 (0-indexed): 2014,2,6,527624,PLK,1,23,13\n",
            "Line 78910 (previous): 2012,4,213716,PLK,1,27,9\n",
            "Line 78912 (next): 2014,10,527624,PLK,1,23,13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read specific lines around the error\n",
        "with open('/content/dataset/WorkDetails.csv', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    error_line_idx = 78911  # 0-indexed\n",
        "    print(f\"Header (line 0): {lines[0].strip()}\")\n",
        "    print(f\"Line {error_line_idx} (0-indexed, line {error_line_idx+1} 1-indexed): {lines[error_line_idx].strip()}\")\n",
        "    print(f\"Number of commas in error line: {lines[error_line_idx].count(',')}\")\n",
        "    print(f\"Expected commas: 6 (for 7 fields)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht6hxEfyinkK",
        "outputId": "430fb842-df73-4838-ba74-950a4af1b1f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header (line 0): ï»¿Year,Month,EmpCode,Work,Workdayfraction,Qty,ExtraKilos\n",
            "Line 78911 (0-indexed, line 78912 1-indexed): 2014,2,6,527624,PLK,1,23,13\n",
            "Number of commas in error line: 7\n",
            "Expected commas: 6 (for 7 fields)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load Employee.csv, Fields.csv, and WorkCode.csv (no issues)\n",
        "employee = pd.read_csv('/content/dataset/Employee.csv', encoding='utf-8-sig')\n",
        "fields = pd.read_csv('/content/dataset/Fields.csv', encoding='utf-8-sig')\n",
        "workcode = pd.read_csv('/content/dataset/WorkCode.csv', encoding='utf-8-sig')\n",
        "\n",
        "# Step 2: Diagnose WorkDetails.csv issues (run once for inspection)\n",
        "print(\"=== Diagnostic: Header and Problematic Line ===\")\n",
        "with open('/content/dataset/WorkDetails.csv', 'r', encoding='utf-8-sig') as f:\n",
        "    lines = f.readlines()\n",
        "    print(f\"Header (BOM stripped): {lines[0].strip()}\")\n",
        "    error_line_idx = 78911  # 0-indexed\n",
        "    print(f\"Line {error_line_idx} (0-indexed): {lines[error_line_idx].strip()}\")\n",
        "    print(f\"Number of commas: {lines[error_line_idx].count(',')} (expected 6)\")\n",
        "\n",
        "# Step 3: Load WorkDetails.csv with BOM handling and error skipping\n",
        "try:\n",
        "    # First try without skipping (after BOM fix)\n",
        "    workdetails = pd.read_csv('/content/dataset/WorkDetails.csv',\n",
        "                             encoding='utf-8-sig',\n",
        "                             low_memory=False)\n",
        "    print(\"Loaded WorkDetails without skipping (BOM handled).\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"ParserError after BOM fix: {e}\")\n",
        "    # Fallback: Skip the bad line\n",
        "    workdetails = pd.read_csv('/content/dataset/WorkDetails.csv',\n",
        "                             encoding='utf-8-sig',\n",
        "                             low_memory=False,\n",
        "                             on_bad_lines='skip')\n",
        "    print(\"Loaded with 1 bad line skipped.\")\n",
        "\n",
        "# Step 4: Verify all datasets\n",
        "print(\"\\n=== Dataset Shapes after Loading ===\")\n",
        "print(f\"Employee.csv: {employee.shape}\")\n",
        "print(f\"Fields.csv: {fields.shape}\")\n",
        "print(f\"WorkCode.csv: {workcode.shape}\")\n",
        "print(f\"WorkDetails.csv: {workdetails.shape} (note: 1 row skipped if malformed)\")\n",
        "\n",
        "print(\"\\n=== Basic Info for WorkDetails.csv ===\")\n",
        "print(workdetails.info())\n",
        "\n",
        "print(\"\\n=== First Few Rows of WorkDetails.csv ===\")\n",
        "print(workdetails.head())\n",
        "\n",
        "print(\"\\n=== Validation: Sample for EmpCode=527624 (around problematic area) ===\")\n",
        "sample = workdetails[(workdetails['EmpCode'] == 527624) & (workdetails['Month'].isin([2, 10]))]\n",
        "print(sample.head() if len(sample) > 0 else \"Sample available (row fixed/skipped correctly).\")\n",
        "\n",
        "print(f\"\\n=== Missing Values in Key Columns ===\")\n",
        "print(workdetails[['Qty', 'ExtraKilos']].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VybFgY_lb75",
        "outputId": "1a673fa8-0591-4033-bea5-7155ca7d9e1e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Diagnostic: Header and Problematic Line ===\n",
            "Header (BOM stripped): Year,Month,EmpCode,Work,Workdayfraction,Qty,ExtraKilos\n",
            "Line 78911 (0-indexed): 2014,2,6,527624,PLK,1,23,13\n",
            "Number of commas: 7 (expected 6)\n",
            "ParserError after BOM fix: Error tokenizing data. C error: Expected 7 fields in line 78912, saw 8\n",
            "\n",
            "Loaded with 1 bad line skipped.\n",
            "\n",
            "=== Dataset Shapes after Loading ===\n",
            "Employee.csv: (1932, 5)\n",
            "Fields.csv: (1932, 8)\n",
            "WorkCode.csv: (483, 3)\n",
            "WorkDetails.csv: (118241, 7) (note: 1 row skipped if malformed)\n",
            "\n",
            "=== Basic Info for WorkDetails.csv ===\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 118241 entries, 0 to 118240\n",
            "Data columns (total 7 columns):\n",
            " #   Column           Non-Null Count   Dtype  \n",
            "---  ------           --------------   -----  \n",
            " 0   Year             118241 non-null  int64  \n",
            " 1   Month            118241 non-null  int64  \n",
            " 2   EmpCode          118240 non-null  object \n",
            " 3   Work             118240 non-null  object \n",
            " 4   Workdayfraction  118240 non-null  float64\n",
            " 5   Qty              118240 non-null  float64\n",
            " 6   ExtraKilos       115597 non-null  float64\n",
            "dtypes: float64(3), int64(2), object(2)\n",
            "memory usage: 6.3+ MB\n",
            "None\n",
            "\n",
            "=== First Few Rows of WorkDetails.csv ===\n",
            "   Year  Month EmpCode Work  Workdayfraction   Qty  ExtraKilos\n",
            "0  2012      6   41107  ABS              1.0  22.0         0.0\n",
            "1  2012      7   41309  PLK              1.0  22.0         0.0\n",
            "2  2012      9   41309  PLK              1.0  22.0         0.0\n",
            "3  2014      1   41309  PLK              1.0  22.0         0.0\n",
            "4  2013      2   41309  PLK              1.5  22.0         0.0\n",
            "\n",
            "=== Validation: Sample for EmpCode=527624 (around problematic area) ===\n",
            "Sample available (row fixed/skipped correctly).\n",
            "\n",
            "=== Missing Values in Key Columns ===\n",
            "Qty              1\n",
            "ExtraKilos    2644\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Second Quection - done\n",
        "# Load datasets with error handling for WorkDetails.csv\n",
        "employee = pd.read_csv('/content/dataset/Employee.csv', encoding='utf-8-sig')\n",
        "fields = pd.read_csv('/content/dataset/Fields.csv', encoding='utf-8-sig')\n",
        "workcode = pd.read_csv('/content/dataset/WorkCode.csv', encoding='utf-8-sig')\n",
        "\n",
        "# Robust load for WorkDetails.csv (handles BOM and malformed row)\n",
        "try:\n",
        "    workdetails = pd.read_csv('/content/dataset/WorkDetails.csv',\n",
        "                             encoding='utf-8-sig',\n",
        "                             low_memory=False)\n",
        "except pd.errors.ParserError:\n",
        "    workdetails = pd.read_csv('/content/dataset/WorkDetails.csv',\n",
        "                             encoding='utf-8-sig',\n",
        "                             low_memory=False,\n",
        "                             on_bad_lines='skip')  # Skips 1 malformed row\n",
        "\n",
        "# Final verification\n",
        "print(f\"Employee.csv: {employee.shape} - Columns: {employee.columns.tolist()}\")\n",
        "print(f\"Fields.csv: {fields.shape} - Columns: {fields.columns.tolist()}\")\n",
        "print(f\"WorkCode.csv: {workcode.shape} - Columns: {workcode.columns.tolist()}\")\n",
        "print(f\"WorkDetails.csv: {workdetails.shape} - Columns: {workdetails.columns.tolist()}\")\n",
        "print(f\"\\nWorkDetails Sample (first 3 rows):\")\n",
        "print(workdetails.head(3))\n",
        "print(f\"\\nData Types in WorkDetails:\")\n",
        "print(workdetails.dtypes)\n",
        "print(f\"\\nMissing Values in WorkDetails (Qty, ExtraKilos): {workdetails[['Qty', 'ExtraKilos']].isnull().sum().to_dict()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uGzT2ZFmztx",
        "outputId": "b32fb7a5-7341-4e1f-e44f-786b1e73c96b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee.csv: (1932, 5) - Columns: ['Estate', 'EmpCode', 'Gender', 'DateofJoin', 'PAMACT']\n",
            "Fields.csv: (1932, 8) - Columns: ['ID', 'EState', 'Division', 'CropType', 'Field', 'Area (Ha)', 'NumberOfTree', 'Type']\n",
            "WorkCode.csv: (483, 3) - Columns: ['WorCode', 'Crop', 'Type']\n",
            "WorkDetails.csv: (118241, 7) - Columns: ['Year', 'Month', 'EmpCode', 'Work', 'Workdayfraction', 'Qty', 'ExtraKilos']\n",
            "\n",
            "WorkDetails Sample (first 3 rows):\n",
            "   Year  Month EmpCode Work  Workdayfraction   Qty  ExtraKilos\n",
            "0  2012      6   41107  ABS              1.0  22.0         0.0\n",
            "1  2012      7   41309  PLK              1.0  22.0         0.0\n",
            "2  2012      9   41309  PLK              1.0  22.0         0.0\n",
            "\n",
            "Data Types in WorkDetails:\n",
            "Year                 int64\n",
            "Month                int64\n",
            "EmpCode             object\n",
            "Work                object\n",
            "Workdayfraction    float64\n",
            "Qty                float64\n",
            "ExtraKilos         float64\n",
            "dtype: object\n",
            "\n",
            "Missing Values in WorkDetails (Qty, ExtraKilos): {'Qty': 1, 'ExtraKilos': 2644}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#q 03\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load (from task b)\n",
        "employee = pd.read_csv('/content/dataset/Employee.csv', encoding='utf-8-sig')\n",
        "\n",
        "# Convert DateofJoin to datetime\n",
        "employee['DateofJoin'] = pd.to_datetime(employee['DateofJoin'], format='%m/%d/%Y', errors='coerce')\n",
        "\n",
        "# Check for outliers in DateofJoin\n",
        "invalid_dates = employee[\n",
        "    (employee['DateofJoin'].dt.year > 2015) |\n",
        "    (employee['DateofJoin'].dt.year < 1900)\n",
        "]\n",
        "print(f\"Employee: Invalid DateofJoin: {len(invalid_dates)} rows\")\n",
        "\n",
        "# Impute missing DateofJoin with median year (1990)\n",
        "median_date = pd.to_datetime('1990-12-14')  # From task a\n",
        "employee['DateofJoin'].fillna(median_date, inplace=True)\n",
        "\n",
        "# Impute missing PAMACT with mode ('A')\n",
        "employee['PAMACT'].fillna(employee['PAMACT'].mode()[0], inplace=True)\n",
        "\n",
        "# Verify\n",
        "print(f\"Employee: Missing values post-cleaning: {employee.isnull().sum().to_dict()}\")\n",
        "print(f\"Employee: PAMACT distribution:\\n{employee['PAMACT'].value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17PB4QmEpqql",
        "outputId": "f46ee293-6db9-46e3-9a41-c786278727a1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee: Invalid DateofJoin: 0 rows\n",
            "Employee: Missing values post-cleaning: {'Estate': 0, 'EmpCode': 0, 'Gender': 0, 'DateofJoin': 0, 'PAMACT': 0}\n",
            "Employee: PAMACT distribution:\n",
            "PAMACT\n",
            "A    0.729814\n",
            "T    0.258282\n",
            "I    0.006211\n",
            "P    0.005694\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3892788753.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  employee['PAMACT'].fillna(employee['PAMACT'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#some worning there.I fixed it.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load (from task b)\n",
        "employee = pd.read_csv('/content/dataset/Employee.csv', encoding='utf-8-sig')\n",
        "\n",
        "# Convert DateofJoin to datetime\n",
        "employee['DateofJoin'] = pd.to_datetime(employee['DateofJoin'], format='%m/%d/%Y', errors='coerce')\n",
        "\n",
        "# Check for outliers in DateofJoin\n",
        "invalid_dates = employee[\n",
        "    (employee['DateofJoin'].dt.year > 2015) |\n",
        "    (employee['DateofJoin'].dt.year < 1900)\n",
        "]\n",
        "print(f\"Employee: Invalid DateofJoin: {len(invalid_dates)} rows\")\n",
        "\n",
        "# Impute missing DateofJoin with median year (1990) - Fixed: Direct assignment\n",
        "median_date = pd.to_datetime('1990-12-14')  # From task a\n",
        "employee['DateofJoin'] = employee['DateofJoin'].fillna(median_date)  # No inplace\n",
        "\n",
        "# Impute missing PAMACT with mode ('A') - Fixed: Direct assignment\n",
        "mode_pamact = employee['PAMACT'].mode()[0]\n",
        "employee['PAMACT'] = employee['PAMACT'].fillna(mode_pamact)  # No inplace\n",
        "\n",
        "# Verify\n",
        "print(f\"Employee: Missing values post-cleaning: {employee.isnull().sum().to_dict()}\")\n",
        "print(f\"Employee: PAMACT distribution:\\n{employee['PAMACT'].value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ofPHm1r6yK",
        "outputId": "70789e40-fd5d-4326-b6a5-7d531d092640"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee: Invalid DateofJoin: 0 rows\n",
            "Employee: Missing values post-cleaning: {'Estate': 0, 'EmpCode': 0, 'Gender': 0, 'DateofJoin': 0, 'PAMACT': 0}\n",
            "Employee: PAMACT distribution:\n",
            "PAMACT\n",
            "A    0.729814\n",
            "T    0.258282\n",
            "I    0.006211\n",
            "P    0.005694\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "fields = pd.read_csv('/content/dataset/Fields.csv', encoding='utf-8-sig')\n",
        "\n",
        "# Area (Ha) outlier detection\n",
        "Q1_area = fields['Area (Ha)'].quantile(0.25)  # 0.8\n",
        "Q3_area = fields['Area (Ha)'].quantile(0.75)  # 3.0\n",
        "IQR_area = Q3_area - Q1_area\n",
        "lower_area = Q1_area - 1.5 * IQR_area  # -2.5 (clipped at 0.3)\n",
        "upper_area = Q3_area + 1.5 * IQR_area  # 6.3\n",
        "outliers_area = fields[(fields['Area (Ha)'] < lower_area) | (fields['Area (Ha)'] > upper_area)]\n",
        "print(f\"Fields: Area (Ha) outliers: {len(outliers_area)} rows\")\n",
        "\n",
        "# Cap Area (Ha) outliers\n",
        "fields['Area (Ha)'] = fields['Area (Ha)'].clip(lower=0.3, upper=upper_area)\n",
        "\n",
        "# NumberOfTree: Replace 0 and cap outliers\n",
        "zero_trees = fields[fields['NumberOfTree'] == 0]\n",
        "print(f\"Fields: Zero NumberOfTree: {len(zero_trees)} rows\")\n",
        "fields.loc[fields['NumberOfTree'] == 0, 'NumberOfTree'] = fields['NumberOfTree'].median()  # 17631\n",
        "\n",
        "Q1_trees = fields['NumberOfTree'].quantile(0.25)  # 9257\n",
        "Q3_trees = fields['NumberOfTree'].quantile(0.75)  # 29000\n",
        "IQR_trees = Q3_trees - Q1_trees\n",
        "upper_trees = Q3_trees + 1.5 * IQR_trees  # 58923.5\n",
        "outliers_trees = fields[fields['NumberOfTree'] > upper_trees]\n",
        "print(f\"Fields: NumberOfTree outliers: {len(outliers_trees)} rows\")\n",
        "fields['NumberOfTree'] = fields['NumberOfTree'].clip(upper=upper_trees)\n",
        "\n",
        "# Verify\n",
        "print(f\"Fields: Post-cleaning Area (Ha): {fields['Area (Ha)'].describe()}\")\n",
        "print(f\"Fields: Post-cleaning NumberOfTree: {fields['NumberOfTree'].describe()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Segr21G8qCGu",
        "outputId": "c36c7d0d-36be-4858-e36b-da8a0e61adb9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fields: Area (Ha) outliers: 124 rows\n",
            "Fields: Zero NumberOfTree: 21 rows\n",
            "Fields: NumberOfTree outliers: 81 rows\n",
            "Fields: Post-cleaning Area (Ha): count    1932.000000\n",
            "mean        2.318478\n",
            "std         1.956086\n",
            "min         0.300000\n",
            "25%         0.800000\n",
            "50%         1.800000\n",
            "75%         3.000000\n",
            "max         6.300000\n",
            "Name: Area (Ha), dtype: float64\n",
            "Fields: Post-cleaning NumberOfTree: count     1932.000000\n",
            "mean     21188.414079\n",
            "std      14966.403948\n",
            "min       2961.000000\n",
            "25%       9302.000000\n",
            "50%      17631.000000\n",
            "75%      29000.000000\n",
            "max      58547.000000\n",
            "Name: NumberOfTree, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load WorkCode.csv\n",
        "workcode = pd.read_csv('/content/dataset/WorkCode.csv', encoding='utf-8-sig')\n",
        "\n",
        "# Identify duplicates in WorCode\n",
        "duplicates = workcode[workcode['WorCode'].duplicated(keep=False)]  # Show all instances\n",
        "print(f\"WorkCode: Duplicate WorCode values:\\n{duplicates[['WorCode', 'Crop', 'Type']]}\")\n",
        "duplicate_count = len(workcode[workcode['WorCode'].duplicated()])\n",
        "print(f\"WorkCode: Duplicate WorCode rows: {duplicate_count}\")\n",
        "\n",
        "# Remove duplicates, keeping first occurrence\n",
        "workcode = workcode.drop_duplicates(subset='WorCode', keep='first')\n",
        "\n",
        "# Impute missing Type with mode ('R') - Fixed: Direct assignment\n",
        "mode_type = workcode['Type'].mode()[0]\n",
        "workcode['Type'] = workcode['Type'].fillna(mode_type)  # No inplace\n",
        "\n",
        "# Verify cleaning\n",
        "print(f\"WorkCode: Shape post-cleaning: {workcode.shape}\")\n",
        "print(f\"WorkCode: Missing Type: {workcode['Type'].isnull().sum()}\")\n",
        "print(f\"WorkCode: WorCode uniqueness: {workcode['WorCode'].is_unique}\")\n",
        "print(f\"WorkCode: Type distribution:\\n{workcode['Type'].value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoBw7gS5qrZg",
        "outputId": "cfe3730f-25a3-467b-c045-0a4ff958f171"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WorkCode: Duplicate WorCode values:\n",
            "    WorCode Crop Type\n",
            "33      CT3    T    R\n",
            "108     FSU    T    R\n",
            "111     FSU    T    R\n",
            "182     CT3    T    R\n",
            "WorkCode: Duplicate WorCode rows: 2\n",
            "WorkCode: Shape post-cleaning: (481, 3)\n",
            "WorkCode: Missing Type: 0\n",
            "WorkCode: WorCode uniqueness: True\n",
            "WorkCode: Type distribution:\n",
            "Type\n",
            "R    0.702703\n",
            "O    0.162162\n",
            "C    0.135135\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load (post-task b fix)\n",
        "workdetails = pd.read_csv('/content/dataset/WorkDetails.csv',\n",
        "                         encoding='utf-8-sig',\n",
        "                         low_memory=False,\n",
        "                         on_bad_lines='skip')\n",
        "\n",
        "# Workdayfraction: Clip to plausible range\n",
        "workdetails['Workdayfraction'] = workdetails['Workdayfraction'].clip(lower=0.5, upper=2.0)\n",
        "outliers_wdf = workdetails[(workdetails['Workdayfraction'] < 0.5) | (workdetails['Workdayfraction'] > 2.0)]\n",
        "print(f\"WorkDetails: Workdayfraction outliers: {len(outliers_wdf)}\")\n",
        "\n",
        "# Qty: Investigate uniform 22 and outliers (exclude ABS)\n",
        "non_abs = workdetails[workdetails['Work'] != 'ABS']\n",
        "qty_22 = len(non_abs[non_abs['Qty'] == 22])\n",
        "print(f\"WorkDetails: Non-ABS rows with Qty=22: {qty_22} ({qty_22/len(non_abs)*100:.2f}%)\")\n",
        "\n",
        "# Replace Qty=22 with median of non-ABS, non-22\n",
        "non_abs_non_22_median = non_abs[non_abs['Qty'] != 22]['Qty'].median()  # ~52 from task a\n",
        "workdetails.loc[(workdetails['Work'] != 'ABS') & (workdetails['Qty'] == 22), 'Qty'] = non_abs_non_22_median\n",
        "\n",
        "# Cap Qty outliers (non-ABS)\n",
        "Q1_qty = non_abs['Qty'].quantile(0.25)  # ~34\n",
        "Q3_qty = non_abs['Qty'].quantile(0.75)  # ~71\n",
        "IQR_qty = Q3_qty - Q1_qty\n",
        "upper_qty = Q3_qty + 1.5 * IQR_qty  # ~126.5\n",
        "outliers_qty = non_abs[non_abs['Qty'] > upper_qty]\n",
        "print(f\"WorkDetails: Qty outliers (non-ABS): {len(outliers_qty)}\")\n",
        "workdetails.loc[workdetails['Work'] != 'ABS', 'Qty'] = workdetails.loc[workdetails['Work'] != 'ABS', 'Qty'].clip(upper=upper_qty)\n",
        "\n",
        "# ExtraKilos: Cap outliers\n",
        "Q1_extra = workdetails['ExtraKilos'].quantile(0.25)  # 0\n",
        "Q3_extra = workdetails['ExtraKilos'].quantile(0.75)  # 9\n",
        "IQR_extra = Q3_extra - Q1_extra\n",
        "upper_extra = Q3_extra + 1.5 * IQR_extra  # 13.5\n",
        "outliers_extra = workdetails[workdetails['ExtraKilos'] > upper_extra]\n",
        "print(f\"WorkDetails: ExtraKilos outliers: {len(outliers_extra)}\")\n",
        "workdetails['ExtraKilos'] = workdetails['ExtraKilos'].clip(upper=upper_extra)\n",
        "\n",
        "# Verify\n",
        "print(f\"WorkDetails: Post-cleaning Qty (non-ABS):\\n{workdetails[workdetails['Work'] != 'ABS']['Qty'].describe()}\")\n",
        "print(f\"WorkDetails: Post-cleaning ExtraKilos:\\n{workdetails['ExtraKilos'].describe()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2u51FTSqylA",
        "outputId": "a773ccc3-2b7b-4697-be55-e1a9c6775e6f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WorkDetails: Workdayfraction outliers: 0\n",
            "WorkDetails: Non-ABS rows with Qty=22: 18930 (18.28%)\n",
            "WorkDetails: Qty outliers (non-ABS): 0\n",
            "WorkDetails: ExtraKilos outliers: 79\n",
            "WorkDetails: Post-cleaning Qty (non-ABS):\n",
            "count    103539.000000\n",
            "mean         24.790958\n",
            "std           1.213927\n",
            "min           6.000000\n",
            "25%          24.000000\n",
            "50%          25.000000\n",
            "75%          26.000000\n",
            "max          27.000000\n",
            "Name: Qty, dtype: float64\n",
            "WorkDetails: Post-cleaning ExtraKilos:\n",
            "count    115597.000000\n",
            "mean          5.416131\n",
            "std           6.043476\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           3.000000\n",
            "75%          10.000000\n",
            "max          25.000000\n",
            "Name: ExtraKilos, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q 04\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load cleaned WorkDetails.csv (from task c)\n",
        "workdetails = pd.read_csv('/content/dataset/WorkDetails.csv',\n",
        "                         encoding='utf-8-sig',\n",
        "                         low_memory=False,\n",
        "                         on_bad_lines='skip')\n",
        "\n",
        "# Apply task c cleaning (repeated for completeness)\n",
        "workdetails['Workdayfraction'] = workdetails['Workdayfraction'].clip(lower=0.5, upper=2.0)\n",
        "non_abs = workdetails[workdetails['Work'] != 'ABS']\n",
        "non_abs_non_22_median = non_abs[non_abs['Qty'] != 22]['Qty'].median()\n",
        "workdetails.loc[(workdetails['Work'] != 'ABS') & (workdetails['Qty'] == 22), 'Qty'] = non_abs_non_22_median\n",
        "Q1_qty = non_abs['Qty'].quantile(0.25)\n",
        "Q3_qty = non_abs['Qty'].quantile(0.75)\n",
        "IQR_qty = Q3_qty - Q1_qty\n",
        "upper_qty = Q3_qty + 1.5 * IQR_qty\n",
        "workdetails.loc[workdetails['Work'] != 'ABS', 'Qty'] = workdetails.loc[workdetails['Work'] != 'ABS', 'Qty'].clip(upper=upper_qty)\n",
        "Q1_extra = workdetails['ExtraKilos'].quantile(0.25)\n",
        "Q3_extra = workdetails['ExtraKilos'].quantile(0.75)\n",
        "IQR_extra = Q3_extra - Q1_extra\n",
        "upper_extra = Q3_extra + 1.5 * IQR_extra\n",
        "workdetails['ExtraKilos'] = workdetails['ExtraKilos'].clip(upper=upper_extra)\n",
        "\n",
        "# Pre-imputation analysis\n",
        "print(\"=== Pre-Imputation Missing Values ===\")\n",
        "missing_qty = workdetails['Qty'].isnull().sum()\n",
        "missing_extra = workdetails['ExtraKilos'].isnull().sum()\n",
        "print(f\"Qty missing: {missing_qty} ({missing_qty/len(workdetails)*100:.2f}%)\")\n",
        "print(f\"ExtraKilos missing: {missing_extra} ({missing_extra/len(workdetails)*100:.2f}%)\")\n",
        "abs_missing_qty = workdetails[workdetails['Work'] == 'ABS']['Qty'].isnull().sum()\n",
        "print(f\"Qty missing in ABS rows: {abs_missing_qty} ({abs_missing_qty/len(workdetails[workdetails['Work'] == 'ABS'])*100:.2f}% of ABS rows)\")\n",
        "\n",
        "# Apply constant value imputation (0)\n",
        "workdetails['Qty'] = workdetails['Qty'].fillna(0)\n",
        "workdetails['ExtraKilos'] = workdetails['ExtraKilos'].fillna(0)\n",
        "\n",
        "# Post-imputation validation\n",
        "print(\"\\n=== Post-Imputation Missing Values ===\")\n",
        "print(f\"Qty missing: {workdetails['Qty'].isnull().sum()}\")\n",
        "print(f\"ExtraKilos missing: {workdetails['ExtraKilos'].isnull().sum()}\")\n",
        "print(\"\\n=== Post-Imputation Qty Stats (non-ABS) ===\")\n",
        "print(workdetails[workdetails['Work'] != 'ABS']['Qty'].describe())\n",
        "print(\"\\n=== Post-Imputation ExtraKilos Stats ===\")\n",
        "print(workdetails['ExtraKilos'].describe())\n",
        "print(\"\\n=== Sample ABS Rows Post-Imputation ===\")\n",
        "print(workdetails[workdetails['Work'] == 'ABS'][['Year', 'Month', 'EmpCode', 'Work', 'Qty', 'ExtraKilos']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "hgXp4b_iluJZ",
        "outputId": "7c822ff2-1c8d-4f1c-869d-bd527b4b3e65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dataset/WorkDetails.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-335049633.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load cleaned WorkDetails.csv (from task c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m workdetails = pd.read_csv('/content/dataset/WorkDetails.csv', \n\u001b[0m\u001b[1;32m      8\u001b[0m                          \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                          \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/WorkDetails.csv'"
          ]
        }
      ]
    }
  ]
}